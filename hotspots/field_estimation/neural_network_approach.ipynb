{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9b97bc-6797-447a-87e3-2e7120348f8a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ea99dfa-723b-43d9-be0b-90c9574795a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-24 14:53:07.356734: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-24 14:53:07.358853: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-24 14:53:07.402828: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-24 14:53:07.404093: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-24 14:53:10.917723: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import pytz\n",
    "import argparse\n",
    "# import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from geopy import distance\n",
    "import datetime\n",
    "import tilemapbase\n",
    "from copy import deepcopy\n",
    "import pickle as pkl\n",
    "from PIL import Image\n",
    "import skimage.measure\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "os.environ['PYTHONWARNINGS']='ignore'\n",
    "import hyperopt\n",
    "from joblib import Parallel, delayed\n",
    "import random\n",
    "random.seed(42)\n",
    "import scipy\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.interpolate import CubicSpline\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "import sklearn.gaussian_process.kernels as kernels\n",
    "import gpflow\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88917b23-0f33-4f13-943a-33af73ffa13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'combined'\n",
    "sensor = 'pm25'\n",
    "res_time = '1H'\n",
    "filepath_root = '/scratch/ab9738/pollution_with_sensors/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d866369-26a0-4043-8a21-f937dede4f7e",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b091ffb3-4bf2-4386-8c54-27b6312a5a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_data_kai = filepath_root+'data/kaiterra/kaiterra_fieldeggid_{}_current_panel.csv'.format(res_time)\n",
    "filepath_data_gov = filepath_root+'data/govdata/govdata_{}_current.csv'.format(res_time)\n",
    "filepath_locs_kai = filepath_root+'data/kaiterra/kaiterra_locations.csv'\n",
    "filepath_locs_gov = filepath_root+'data/govdata/govdata_locations.csv'\n",
    "\n",
    "locs_kai = pd.read_csv(filepath_locs_kai, index_col=[0])\n",
    "locs_kai['Type'] = 'Kaiterra'\n",
    "locs_gov = pd.read_csv(filepath_locs_gov, index_col=[0])\n",
    "locs_gov['Type'] = 'Govt'\n",
    "locs = pd.merge(locs_kai, locs_gov, how='outer',\\\n",
    "                on=['Monitor ID', 'Latitude', 'Longitude', 'Location', 'Type'], copy=False)\n",
    "data_kai = pd.read_csv(filepath_data_kai, index_col=[0,1], parse_dates=True)[sensor]\n",
    "data_gov = pd.read_csv(filepath_data_gov, index_col=[0,1], parse_dates=True)[sensor]\n",
    "data = pd.concat([data_kai, data_gov], axis=0, copy=False)\n",
    "data.replace(0,np.nan,inplace=True)\n",
    "\n",
    "start_dt = data.index.levels[1][0]\n",
    "end_dt = data.index.levels[1][-1]\n",
    "\n",
    "if start_dt.tzname != 'IST':\n",
    "        if start_dt.tzinfo is None:\n",
    "            start_dt = start_dt.tz_localize('UTC')\n",
    "        start_dt = start_dt.tz_convert(pytz.FixedOffset(330))\n",
    "    \n",
    "if end_dt.tzname != 'IST':\n",
    "    if end_dt.tzinfo is None: \n",
    "        end_dt = end_dt.tz_localize('UTC')\n",
    "    end_dt = end_dt.tz_convert(pytz.FixedOffset(330))\n",
    "\n",
    "# now, filter through the start and end dates\n",
    "data.sort_index(inplace=True)\n",
    "data = data.loc[(slice(None), slice(start_dt, end_dt))]\n",
    "\n",
    "if(source=='govdata'):\n",
    "    df = data_gov.unstack(level=0)\n",
    "elif(source=='kaiterra'):\n",
    "    df = data_kai.unstack(level=0)\n",
    "else:\n",
    "    df = data.unstack(level=0)\n",
    "distances = pd.read_csv('/scratch/ab9738/pollution_with_sensors/data/combined_distances.csv', index_col=[0])\n",
    "distances = distances.loc[df.columns, df.columns]\n",
    "distances[distances == 0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2df74290-120a-4c6a-8f56-3a2ea1cb92ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.log(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78d6838-4464-404d-b459-cd76c86c2bbe",
   "metadata": {},
   "source": [
    "Spline correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39070c0b-6c76-4107-9f10-7991a3523b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sens = np.log(data).to_frame().reset_index()\n",
    "\n",
    "sens['hour_of_day'] = sens['timestamp_round'].apply(lambda x: x.hour)\n",
    "\n",
    "spline = sens.groupby(['field_egg_id', 'hour_of_day']).mean()['pm25'].reset_index()\n",
    "spline_avg = sens.groupby(['hour_of_day']).mean()['pm25'].reset_index()\n",
    "\n",
    "fields = []\n",
    "times = []\n",
    "pm25 = []\n",
    "for i in np.unique(spline['field_egg_id']):\n",
    "    s_i = spline[spline['field_egg_id']==i]\n",
    "    x = s_i['hour_of_day'].values\n",
    "    y = [t for t in s_i['pm25'].values]\n",
    "    c1 = CubicSpline(x[:8],y[:8])\n",
    "    c2 = CubicSpline(x[8:16],y[8:16])\n",
    "    c3 = CubicSpline(x[16:24],y[16:24])\n",
    "    ix = [k/100.0 for k in range(2400)]\n",
    "    iy = list(np.concatenate((c1(ix[:800]),c2(ix[800:1600]),c3(ix[1600:2400]))))\n",
    "    fields += [i]*2400\n",
    "    times += ix\n",
    "    pm25 += iy\n",
    "\n",
    "spline_df = pd.DataFrame((fields, times, pm25)).transpose()\n",
    "\n",
    "spline_df.columns = ['field_egg_id', 'time', 'pm25']\n",
    "\n",
    "hours_in_day = np.arange(24).astype(float)\n",
    "\n",
    "spline_df = spline_df[spline_df['time'].isin(hours_in_day)]\n",
    "\n",
    "spline_mat = np.transpose(spline_df['pm25'].to_numpy().reshape((60,24))).astype(float)\n",
    "\n",
    "spline_df = pd.DataFrame(spline_mat,columns=df.columns)\n",
    "df_full = deepcopy(df)\n",
    "for idx,row in df.iterrows():\n",
    "    df.loc[idx] = row-spline_df.loc[idx.hour]\n",
    "df_spline = df_full-df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07620570-f224-4425-897a-eb5907dca781",
   "metadata": {
    "tags": []
   },
   "source": [
    "Data Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46e140be-7fde-4a73-8201-1af8c621e1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21960/21960 [00:44<00:00, 494.92it/s]\n"
     ]
    }
   ],
   "source": [
    "x_tr, x_ts, y_tr, y_ts, spl_tr, spl_ts = [], [], [], [], [], []\n",
    "for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    lat = locs.loc[df.columns]['Longitude'].values\n",
    "    long = locs.loc[df.columns]['Latitude'].values\n",
    "    val = row.values\n",
    "    \n",
    "    cols = np.array(df.columns)[~np.isnan(val)]\n",
    "    lat = lat[~np.isnan(val)]\n",
    "    long = long[~np.isnan(val)]\n",
    "    val = val[~np.isnan(val)]\n",
    "    \n",
    "    t_year = idx.year\n",
    "    t_month = idx.month\n",
    "    t_day = idx.day\n",
    "    t_hour = idx.hour\n",
    "    t_day_of_week = idx.day_name()\n",
    "\n",
    "    if(len(val)<30):\n",
    "        continue\n",
    "        \n",
    "    x = np.array([[la,lo,t_year, t_month, t_day, t_hour, t_day_of_week] for la, lo in zip(lat, long)])\n",
    "    y = np.array(val)\n",
    "    y = np.expand_dims(y, 1)\n",
    "    \n",
    "    x_train, x_test, y_train, y_test, cols_train, cols_test = train_test_split(\n",
    "        x, y, cols, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    spl_train = spline_df.loc[idx.hour][cols_train].values\n",
    "    spl_train = np.expand_dims(np.array(spl_train),1)\n",
    "    spl_test = spline_df.loc[idx.hour][cols_test].values\n",
    "    spl_test = np.expand_dims(np.array(spl_test),1)\n",
    "    \n",
    "    x_tr.append(x_train)\n",
    "    x_ts.append(x_test)\n",
    "    y_tr.append(y_train)\n",
    "    y_ts.append(y_test)\n",
    "    spl_tr.append(spl_train)\n",
    "    spl_ts.append(spl_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9481cb56-f3d2-4e44-a526-0dce75484a6e",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55a59133-f429-4c18-ba6f-2e80909ef19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = np.concatenate(x_tr)\n",
    "x_ts = np.concatenate(x_ts)\n",
    "y_tr = np.concatenate(y_tr)\n",
    "spl_tr = np.concatenate(spl_tr)\n",
    "y_ts = np.concatenate(y_ts)\n",
    "spl_ts = np.concatenate(spl_ts)\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "ohe.fit(x_tr[:,[2,3,4,6]])\n",
    "# print(ohe.categories_)\n",
    "\n",
    "x_tr = np.concatenate([x_tr[:,[0,1,5]].astype(np.float64),ohe.transform(x_tr[:,[2,3,4,6]]).toarray()], axis=1)\n",
    "x_ts = np.concatenate([x_ts[:,[0,1,5]].astype(np.float64),ohe.transform(x_ts[:,[2,3,4,6]]).toarray()], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ed47f7-644f-4c2c-b3b1-eaa5b2880e34",
   "metadata": {},
   "source": [
    "Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c8880c-ee2b-4675-85bd-ac14e48003f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = MLPRegressor(hidden_layer_sizes=(1024,256,64,16,), random_state=42, max_iter=50, verbose=True)\n",
    "reg_model.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6408641a-9e99-4e74-b997-9e6414c70734",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./mlp_regressor_fulldata.pickle', 'wb') as pickle_file:\n",
    "    pkl.dump(reg_model, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be28462a-7b09-4b36-8cf2-c47b314cc6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./mlp_regressor_fulldata.pickle', 'rb') as pickle_file:\n",
    "    saved_model = pkl.load(pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58f1a8b-680d-45db-92f2-58ce42393698",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad27fc-a366-4a2e-a99b-0c36f134934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = local_model.predict(x_ts)\n",
    "pred = np.expand_dims(pred, axis=1)\n",
    "ape_arr = np.abs(np.exp(y_ts+spl_ts)-np.exp(pred+spl_ts))/np.exp(y_ts+spl_ts)\n",
    "mape = np.mean(ape_arr)\n",
    "mape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f45b94-6589-4298-bddb-46a0b5f89f5e",
   "metadata": {},
   "source": [
    "Row-Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7700c327-8bef-4998-b60b-9bee7bd6c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompute x_tr and others from df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49679d9-1e88-4c12-8a05-f21ac9cf7c18",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wrong as i is not a row anymore but a point. The row is getting filled in from data, making it different from kriging case.\n",
    "# ape = []\n",
    "# ws = 10\n",
    "# for i in tqdm(range(ws,len(x_tr[ws:-ws]))):\n",
    "#     x_train, x_test, y_train, y_test, spl_train, spl_test = x_tr[i-ws:i+ws], x_ts[i], y_tr[i-ws:i+ws], y_ts[i],\\\n",
    "#     spl_tr[i-ws:i+ws], spl_ts[i]\n",
    "#     x_train = x_train+x_ts[i-ws:i]+x_ts[i+1:i+ws]\n",
    "#     y_train = y_train+y_ts[i-ws:i]+y_ts[i+1:i+ws]\n",
    "#     x_train = np.concatenate(x_train)\n",
    "#     y_train = np.concatenate(y_train)\n",
    "    \n",
    "#     x_train = np.concatenate([x_train[:,[0,1,5]].astype(np.float64),ohe.transform(x_train[:,[2,3,4,6]]).toarray()], axis=1)\n",
    "#     x_test = np.concatenate([x_test[:,[0,1,5]].astype(np.float64),ohe.transform(x_test[:,[2,3,4,6]]).toarray()], axis=1)\n",
    "    \n",
    "#     local_model = deepcopy(reg_model)\n",
    "    \n",
    "#     for m in range(5):\n",
    "#         local_model.partial_fit(x_train, y_train)\n",
    "    \n",
    "#     pred = local_model.predict(x_test)\n",
    "#     pred = np.expand_dims(pred, axis=1)\n",
    "    \n",
    "#     ape.append(np.abs((np.exp(y_test+spl_test)-np.exp(pred+spl_test))/np.exp(y_test+spl_test)))\n",
    "    \n",
    "# ape_arr = np.concatenate(ape)\n",
    "# mape = np.mean(ape_arr)\n",
    "# mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc30fc-fda9-45b2-be30-0664efba6a62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
