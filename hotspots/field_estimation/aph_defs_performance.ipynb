{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dacdeaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pytz\n",
    "import argparse\n",
    "# import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from geopy import distance\n",
    "import datetime\n",
    "import tilemapbase\n",
    "from copy import deepcopy\n",
    "import pickle as pkl\n",
    "from PIL import Image\n",
    "import skimage.measure\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "os.environ['PYTHONWARNINGS']='ignore'\n",
    "import hyperopt\n",
    "from joblib import Parallel, delayed\n",
    "import random\n",
    "random.seed(42)\n",
    "import scipy\n",
    "import torch\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "from pykrige.ok3d import OrdinaryKriging3D\n",
    "from pykrige.uk import UniversalKriging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.interpolate import CubicSpline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "941b563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'combined'\n",
    "sensor = 'pm25'\n",
    "res_time = '1H'\n",
    "filepath_root = '/scratch/ab9738/pollution_with_sensors/'\n",
    "\n",
    "filepath_data_kai = filepath_root+'data/kaiterra/kaiterra_fieldeggid_{}_current_panel.csv'.format(res_time)\n",
    "filepath_data_gov = filepath_root+'data/govdata/govdata_{}_current.csv'.format(res_time)\n",
    "filepath_locs_kai = filepath_root+'data/kaiterra/kaiterra_locations.csv'\n",
    "filepath_locs_gov = filepath_root+'data/govdata/govdata_locations.csv'\n",
    "\n",
    "locs_kai = pd.read_csv(filepath_locs_kai, index_col=[0])\n",
    "locs_kai['Type'] = 'Kaiterra'\n",
    "locs_gov = pd.read_csv(filepath_locs_gov, index_col=[0])\n",
    "locs_gov['Type'] = 'Govt'\n",
    "locs = pd.merge(locs_kai, locs_gov, how='outer',\\\n",
    "                on=['Monitor ID', 'Latitude', 'Longitude', 'Location', 'Type'], copy=False)\n",
    "data_kai = pd.read_csv(filepath_data_kai, index_col=[0,1], parse_dates=True)[sensor]\n",
    "data_gov = pd.read_csv(filepath_data_gov, index_col=[0,1], parse_dates=True)[sensor]\n",
    "data = pd.concat([data_kai, data_gov], axis=0, copy=False)\n",
    "data.replace(0,np.nan,inplace=True)\n",
    "\n",
    "start_dt = data.index.levels[1][0]\n",
    "end_dt = data.index.levels[1][-1]\n",
    "\n",
    "if start_dt.tzname != 'IST':\n",
    "        if start_dt.tzinfo is None:\n",
    "            start_dt = start_dt.tz_localize('UTC')\n",
    "        start_dt = start_dt.tz_convert(pytz.FixedOffset(330))\n",
    "    \n",
    "if end_dt.tzname != 'IST':\n",
    "    if end_dt.tzinfo is None: \n",
    "        end_dt = end_dt.tz_localize('UTC')\n",
    "    end_dt = end_dt.tz_convert(pytz.FixedOffset(330))\n",
    "\n",
    "# now, filter through the start and end dates\n",
    "data.sort_index(inplace=True)\n",
    "data = data.loc[(slice(None), slice(start_dt, end_dt))]\n",
    "\n",
    "if(source=='govdata'):\n",
    "    df = data_gov.unstack(level=0)\n",
    "elif(source=='kaiterra'):\n",
    "    df = data_kai.unstack(level=0)\n",
    "else:\n",
    "    df = data.unstack(level=0)\n",
    "distances = pd.read_csv('/scratch/ab9738/pollution_with_sensors/data/combined_distances.csv', index_col=[0])\n",
    "distances = distances.loc[df.columns, df.columns]\n",
    "distances[distances == 0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49b837d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Pusa_IMD'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "352e7111",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m10 = pd.read_csv(\"missing_sensors_0.1.csv\",index_col=[0],parse_dates=True)\n",
    "df_m20 = pd.read_csv(\"missing_sensors_0.2.csv\",index_col=[0],parse_dates=True)\n",
    "df_m30 = pd.read_csv(\"missing_sensors_0.3.csv\",index_col=[0],parse_dates=True)\n",
    "df_m40 = pd.read_csv(\"missing_sensors_0.4.csv\",index_col=[0],parse_dates=True)\n",
    "df_m50 = pd.read_csv(\"missing_sensors_0.5.csv\",index_col=[0],parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4f06449",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing = {}\n",
    "df_missing[10] = df_m10\n",
    "df_missing[20] = df_m20\n",
    "df_missing[30] = df_m30\n",
    "df_missing[40] = df_m40\n",
    "df_missing[50] = df_m50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85ed3825",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i10 = pd.read_csv(\"missing_sensors_interpolated_0.1.csv\",index_col=[0],parse_dates=True)\n",
    "df_i20 = pd.read_csv(\"missing_sensors_interpolated_0.2.csv\",index_col=[0],parse_dates=True)\n",
    "df_i30 = pd.read_csv(\"missing_sensors_interpolated_0.3.csv\",index_col=[0],parse_dates=True)\n",
    "df_i40 = pd.read_csv(\"missing_sensors_interpolated_0.4.csv\",index_col=[0],parse_dates=True)\n",
    "df_i50 = pd.read_csv(\"missing_sensors_interpolated_0.5.csv\",index_col=[0],parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54c672fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interpolated = {}\n",
    "df_interpolated[10] = df_i10\n",
    "df_interpolated[20] = df_i20\n",
    "df_interpolated[30] = df_i30\n",
    "df_interpolated[40] = df_i40\n",
    "df_interpolated[50] = df_i50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092584b5",
   "metadata": {},
   "source": [
    "## APH Paper Hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "688694aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby(df.index.date).mean()\n",
    "df.index = pd.to_datetime(df.index)\n",
    "for key in [10,20,30,40,50]:\n",
    "    df_missing[key] = df_missing[key].groupby(df_missing[key].index.date).mean()\n",
    "    df_interpolated[key] = df_interpolated[key].groupby(df_interpolated[key].index.date).mean()\n",
    "    df_missing[key].index = pd.to_datetime(df_missing[key].index)\n",
    "    df_interpolated[key].index = pd.to_datetime(df_interpolated[key].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17c2fbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_month(df_month):\n",
    "    freq, scale, cons = [], [], []\n",
    "    for sensor in df_month.columns:\n",
    "        if(df_month[sensor].isna().sum()>10):\n",
    "            continue\n",
    "        if(len(df_month[sensor][df_month[sensor]>60])>0.6*len(df_month[sensor].dropna())):\n",
    "            freq.append(sensor)\n",
    "        if(df_month[sensor].mean()>90):\n",
    "            scale.append(sensor)\n",
    "        y = df_month[sensor].dropna()\n",
    "        y = y-60\n",
    "        y = (y>0).astype(int)\n",
    "        y = y * (y.groupby((y != y.shift()).cumsum()).cumcount() + 1)\n",
    "        if(max(y)>=3):\n",
    "            cons.append(sensor)\n",
    "    return(freq,scale,cons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b92572f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_hsps(dataframe): \n",
    "    df_18 = dataframe[dataframe.index.year==2018]\n",
    "    df_19 = dataframe[dataframe.index.year==2019]\n",
    "    df_20 = dataframe[dataframe.index.year==2020]\n",
    "    # year 2018\n",
    "    hsps = {2018:{},2019:{},2020:{}}\n",
    "    for month in range(5,13):\n",
    "        df_month = df_18[df_18.index.month==month]    \n",
    "        hsps[2018][month]=process_month(df_month)\n",
    "\n",
    "\n",
    "    # year 2019\n",
    "    for month in range(1,13):\n",
    "        df_month = df_19[df_19.index.month==month]    \n",
    "        hsps[2019][month]=process_month(df_month)\n",
    "\n",
    "    # year 2020\n",
    "    for month in range(1,10):\n",
    "        df_month = df_20[df_20.index.month==month]    \n",
    "        hsps[2020][month]=process_month(df_month)\n",
    "\n",
    "    return(hsps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1814473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tp_fp_fn(gt, pred):\n",
    "    tp_list, fp_list, fn_list = [], [], []\n",
    "    for k in range(3):\n",
    "        gt_list = gt[k]\n",
    "        pred_list = pred[k]\n",
    "        tp_list += [x for x in pred_list if x in gt_list]\n",
    "        fp_list += [x for x in pred_list if x not in gt_list]\n",
    "        fn_list += [x for x in gt_list if x not in pred_list]\n",
    "    return(tp_list, fp_list, fn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "069821b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = count_hsps(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52d3c9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.9166224110462029\n",
      "1.0 0.8093467870419543\n",
      "1.0 0.7424322889006904\n",
      "1.0 0.6165693043016464\n",
      "1.0 0.5315985130111525\n"
     ]
    }
   ],
   "source": [
    "for key in [10,20,30,40,50]:\n",
    "    pred = count_hsps(df_missing[key])\n",
    "\n",
    "    tp_list, fp_list, fn_list = [],[],[]\n",
    "    for i in gt.keys():\n",
    "        for j in gt[i].keys():\n",
    "            tp, fp, fn = find_tp_fp_fn(gt[i][j], pred[i][j])\n",
    "            tp_list += tp\n",
    "            fp_list += fp\n",
    "            fn_list += fn\n",
    "\n",
    "    precision = len(tp_list)/(len(tp_list)+len(fp_list))\n",
    "    recall = len(tp_list)/(len(tp_list)+len(fn_list))\n",
    "    print(precision,recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e06412cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9978529253891573 0.987254381306426\n",
      "0.9929805615550756 0.9766330323951142\n",
      "0.9918610960390667 0.9707912904938927\n",
      "0.983013698630137 0.9527349973446628\n",
      "0.9791094007696537 0.9458311205523101\n"
     ]
    }
   ],
   "source": [
    "for key in [10,20,30,40,50]:\n",
    "    pred = count_hsps(df_interpolated[key])\n",
    "\n",
    "    tp_list, fp_list, fn_list = [],[],[]\n",
    "    for i in gt.keys():\n",
    "        for j in gt[i].keys():\n",
    "            tp, fp, fn = find_tp_fp_fn(gt[i][j], pred[i][j])\n",
    "            tp_list += tp\n",
    "            fp_list += fp\n",
    "            fn_list += fn\n",
    "\n",
    "    precision = len(tp_list)/(len(tp_list)+len(fp_list))\n",
    "    recall = len(tp_list)/(len(tp_list)+len(fn_list))\n",
    "    print(precision,recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86d0d93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
