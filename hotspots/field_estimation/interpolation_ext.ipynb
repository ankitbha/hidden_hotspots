{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "prop = sys.argv[1]\n",
    "import pytz\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from geopy import distance\n",
    "import datetime\n",
    "import tilemapbase\n",
    "from copy import deepcopy\n",
    "import pickle as pkl\n",
    "from PIL import Image\n",
    "import skimage.measure\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "os.environ['PYTHONWARNINGS']='ignore'\n",
    "from joblib import Parallel, delayed\n",
    "import random\n",
    "random.seed(42)\n",
    "import scipy\n",
    "import torch\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "from pykrige.ok3d import OrdinaryKriging3D\n",
    "from pykrige.uk import UniversalKriging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.interpolate import CubicSpline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'combined'\n",
    "sensor = 'pm25'\n",
    "res_time = '1H'\n",
    "filepath_root = '/scratch/ab9738/pollution_with_sensors/'\n",
    "\n",
    "filepath_data_kai = filepath_root+'data/kaiterra/kaiterra_fieldeggid_{}_current_panel.csv'.format(res_time)\n",
    "filepath_data_gov = filepath_root+'data/govdata/govdata_{}_current.csv'.format(res_time)\n",
    "filepath_locs_kai = filepath_root+'data/kaiterra/kaiterra_locations.csv'\n",
    "filepath_locs_gov = filepath_root+'data/govdata/govdata_locations.csv'\n",
    "\n",
    "locs_kai = pd.read_csv(filepath_locs_kai, index_col=[0])\n",
    "locs_kai['Type'] = 'Kaiterra'\n",
    "locs_gov = pd.read_csv(filepath_locs_gov, index_col=[0])\n",
    "locs_gov['Type'] = 'Govt'\n",
    "locs = pd.merge(locs_kai, locs_gov, how='outer',\\\n",
    "                on=['Monitor ID', 'Latitude', 'Longitude', 'Location', 'Type'], copy=False)\n",
    "data_kai = pd.read_csv(filepath_data_kai, index_col=[0,1], parse_dates=True)[sensor]\n",
    "data_gov = pd.read_csv(filepath_data_gov, index_col=[0,1], parse_dates=True)[sensor]\n",
    "data = pd.concat([data_kai, data_gov], axis=0, copy=False)\n",
    "data.replace(0,np.nan,inplace=True)\n",
    "\n",
    "start_dt = data.index.levels[1][0]\n",
    "end_dt = data.index.levels[1][-1]\n",
    "\n",
    "if start_dt.tzname != 'IST':\n",
    "        if start_dt.tzinfo is None:\n",
    "            start_dt = start_dt.tz_localize('UTC')\n",
    "        start_dt = start_dt.tz_convert(pytz.FixedOffset(330))\n",
    "    \n",
    "if end_dt.tzname != 'IST':\n",
    "    if end_dt.tzinfo is None: \n",
    "        end_dt = end_dt.tz_localize('UTC')\n",
    "    end_dt = end_dt.tz_convert(pytz.FixedOffset(330))\n",
    "\n",
    "# now, filter through the start and end dates\n",
    "data.sort_index(inplace=True)\n",
    "data = data.loc[(slice(None), slice(start_dt, end_dt))]\n",
    "\n",
    "if(source=='govdata'):\n",
    "    df = data_gov.unstack(level=0)\n",
    "elif(source=='kaiterra'):\n",
    "    df = data_kai.unstack(level=0)\n",
    "else:\n",
    "    df = data.unstack(level=0)\n",
    "distances = pd.read_csv('/scratch/ab9738/pollution_with_sensors/data/combined_distances.csv', index_col=[0])\n",
    "distances = distances.loc[df.columns, df.columns]\n",
    "distances[distances == 0] = np.nan\n",
    "\n",
    "df = np.log(df)\n",
    "\n",
    "cols = df.columns.to_list()\n",
    "cols.remove('Pusa_IMD')\n",
    "cols_train, cols_test = train_test_split(cols, test_size=0.1, random_state=42)\n",
    "\n",
    "sens = np.log(data).to_frame().reset_index()\n",
    "\n",
    "sens['hour_of_day'] = sens['timestamp_round'].apply(lambda x: x.hour)\n",
    "\n",
    "spline = sens.groupby(['field_egg_id', 'hour_of_day']).mean()['pm25'].reset_index()\n",
    "spline_avg = sens.groupby(['hour_of_day']).mean()['pm25'].reset_index()\n",
    "\n",
    "fields = []\n",
    "times = []\n",
    "pm25 = []\n",
    "for i in np.unique(spline['field_egg_id']):\n",
    "    s_i = spline[spline['field_egg_id']==i]\n",
    "    x = s_i['hour_of_day'].values\n",
    "    y = [t for t in s_i['pm25'].values]\n",
    "    c1 = CubicSpline(x[:8],y[:8])\n",
    "    c2 = CubicSpline(x[8:16],y[8:16])\n",
    "    c3 = CubicSpline(x[16:24],y[16:24])\n",
    "    ix = [k/100.0 for k in range(2400)]\n",
    "    iy = list(np.concatenate((c1(ix[:800]),c2(ix[800:1600]),c3(ix[1600:2400]))))\n",
    "    fields += [i]*2400\n",
    "    times += ix\n",
    "    pm25 += iy\n",
    "\n",
    "spline_df = pd.DataFrame((fields, times, pm25)).transpose()\n",
    "\n",
    "spline_df.columns = ['field_egg_id', 'time', 'pm25']\n",
    "\n",
    "hours_in_day = np.arange(24).astype(float)\n",
    "\n",
    "spline_df = spline_df[spline_df['time'].isin(hours_in_day)]\n",
    "\n",
    "spline_mat = np.transpose(spline_df['pm25'].to_numpy().reshape((60,24))).astype(float)\n",
    "\n",
    "spline_df = pd.DataFrame(spline_mat,columns=df.columns)\n",
    "spline_df = spline_df.drop(['Pusa_IMD'], axis=1)\n",
    "df = df.drop(['Pusa_IMD'], axis=1)\n",
    "spline_df = spline_df[train_cols].mean(axis=1)\n",
    "df_full = deepcopy(df)\n",
    "for idx,row in df.iterrows():\n",
    "    df.loc[idx] = row-spline_df.loc[idx.hour]\n",
    "df_spline = df_full-df\n",
    "\n",
    "df_interp = deepcopy(df)\n",
    "df_null = deepcopy(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(idx, row):\n",
    "    window_size = 3\n",
    "    i = np.where(np.array(df.index) == idx)[0][0]\n",
    "    df_slice = pd.concat([df[i-window_size:i],df[i+1:i+window_size+1]])\n",
    "    x_win = locs.loc[cols_train]['Longitude'].values\n",
    "    x_win = np.tile(x_win,df_slice.shape[0])\n",
    "    y_win = locs.loc[cols_train]['Latitude'].values\n",
    "    y_win = np.tile(y_win,df_slice.shape[0])    \n",
    "    z_win = np.concatenate([np.arange(i-window_size,i),np.arange(i+1,i+window_size+1)])*0.01\n",
    "    z_win = np.repeat(z_win,len(cols_train))\n",
    "    vals_win = df_slice[cols_train].values.flatten()\n",
    "    x_win = x_win[~np.isnan(vals_win)]\n",
    "    y_win = y_win[~np.isnan(vals_win)]\n",
    "    z_win = z_win[~np.isnan(vals_win)]\n",
    "    vals_win = vals_win[~np.isnan(vals_win)]\n",
    "    \n",
    "    x_train = locs.loc[cols_train]['Longitude'].values\n",
    "    y_train = locs.loc[cols_train]['Latitude'].values\n",
    "    z_train = np.ones_like(x_train)*i*0.01\n",
    "    vals_train = row[cols_train].values\n",
    "    \n",
    "    x_train = x_train[~np.isnan(vals_train)]\n",
    "    y_train = y_train[~np.isnan(vals_train)]\n",
    "    z_train = z_train[~np.isnan(vals_train)]\n",
    "    vals_train = vals_train[~np.isnan(vals_train)]\n",
    "    \n",
    "    x_test = locs.loc[cols_test]['Longitude'].values\n",
    "    y_test = locs.loc[cols_test]['Latitude'].values\n",
    "    z_test = np.ones_like(x_test)*i*0.01\n",
    "    vals_test = row[cols_test].values\n",
    "    \n",
    "    x_train = np.concatenate([x_train,x_win])\n",
    "    y_train = np.concatenate([y_train,y_win])\n",
    "    z_train = np.concatenate([z_train,z_win])\n",
    "    vals_train = np.concatenate([vals_train,vals_win])\n",
    "\n",
    "\n",
    "    OK3D = OrdinaryKriging3D(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        z_train,\n",
    "        vals_train,\n",
    "        variogram_model=\"linear\",\n",
    "        verbose=False,\n",
    "        enable_plotting=False,\n",
    "    )\n",
    "\n",
    "    vals_pred, ss_pred = OK3D.execute(\"points\", x_test, y_test, z_test)\n",
    "    return(idx,cols_test,vals_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignment_record = Parallel(n_jobs=2)(delayed(process_row)(idx,row) for idx,row in df[4:-4].iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
