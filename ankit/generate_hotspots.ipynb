{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "658aa98e",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b4b784b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import pytz\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from geopy import distance\n",
    "import datetime\n",
    "import tilemapbase\n",
    "from copy import deepcopy\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97d310d",
   "metadata": {},
   "source": [
    "# Relative Spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf332c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal hotspot\n",
    "def is_win_thigh(win, ff):\n",
    "    # \"win\" is of type numpy.ndarray\n",
    "    c_ii = (len(win)-1)//2\n",
    "    c = win[c_ii]\n",
    "    m = np.maximum(win[:c_ii].max(), win[c_ii+1:].max())\n",
    "    r = c >= (1 + ff)*m\n",
    "    return r\n",
    "\n",
    "\n",
    "def is_win_tlow(win, ff):\n",
    "    # \"win\" is of type numpy.ndarray\n",
    "    c_ii = (len(win)-1)//2\n",
    "    c = win[c_ii]\n",
    "    m = np.minimum(win[:c_ii].min(), win[c_ii+1:].min())\n",
    "    r = c <= ff*m\n",
    "    return r\n",
    "\n",
    "\n",
    "def get_spikes(data, sensor, params, locs):\n",
    "\n",
    "    # get data and distances; data is expected to be a pandas.Series,\n",
    "    # not pandas.DataFrame, containing only one column (either pm25 or\n",
    "    # pm10)\n",
    "    df = data.unstack(level=0)\n",
    "    distances = pd.read_csv('/scratch/ab9738/epod-nyu-delhi-pollution/data/combined_distances.csv', index_col=[0])\n",
    "\n",
    "    # select only the locations that are in the data\n",
    "    distances = distances.loc[df.columns, df.columns]\n",
    "\n",
    "    # invalidate diagonal entries so that sensor M does not get\n",
    "    # counted in the M's radius\n",
    "    distances[distances == 0] = np.nan\n",
    "\n",
    "    # res: three digit entries 'abc' or NaN, where a/b/c = 1 or 9\n",
    "    #\n",
    "    # a == 9 => thigh, a == 1 => tlow\n",
    "    # b == 9 => shigh, b == 1 => slow\n",
    "    # c == 9 => jhigh, c == 1 => jlow\n",
    "    res = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "\n",
    "    # (1) WINDOW HOTSPOTS\n",
    "\n",
    "    # **Temporal Window Hotspot**: a timestamp is marked as a temporal\n",
    "    # window hotspot if the value at that time is greater/lesser than\n",
    "    # a window (radius \"wtr\") around it by a threshold fraction \"wttf\"\n",
    "    wts = 2*params['wtr'] + 1\n",
    "    rolling_wt = df.rolling(wts, min_periods=wts, center=True)\n",
    "    res_win_thigh = rolling_wt.apply(is_win_thigh, raw=True, args=(params['wttf'],))\n",
    "    res_win_tlow = rolling_wt.apply(is_win_tlow, raw=True, args=(params['wttf'],))\n",
    "    res[res_win_thigh == 1] = 900\n",
    "    res[res_win_tlow == 1] = 100\n",
    "\n",
    "    # **Spatial Window Hotspot**: A location is marked as a\n",
    "    # spatial window hotspot if, at a given time, the value at\n",
    "    # that location is greater/lesser than the max of values in a\n",
    "    # radius (\"wsr\") around it by a threshold frac \"wstf\"\n",
    "    res_win_shigh = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "    res_win_slow = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "    for mid in df.columns:\n",
    "        neighborhood = (distances.loc[mid] <= params['wsr'] * 1000)\n",
    "        neighborhood_max = df.loc[:,neighborhood].max(axis=1)\n",
    "        neighborhood_min = df.loc[:,neighborhood].min(axis=1)\n",
    "        res_win_shigh.loc[:, mid] = (df[mid] > ((1 + params['wstf']) * neighborhood_max))\n",
    "        res_win_shigh.loc[neighborhood_max.isna() | df[mid].isna(), mid] = np.nan\n",
    "        res_win_slow.loc[:, mid]  = (df[mid] < (params['wstf'] * neighborhood_min))\n",
    "        res_win_slow.loc[neighborhood_min.isna() | df[mid].isna(), mid] = np.nan\n",
    "    res[(res_win_shigh == 1) & res.notna()] += 90\n",
    "    res[(res_win_shigh == 1) & res.isna()] = 90\n",
    "    res[(res_win_slow == 1) & res.notna()] += 10\n",
    "    res[(res_win_slow == 1) & res.isna()] = 10\n",
    "\n",
    "    # (2) JUMP HOTSPOTS\n",
    "\n",
    "    # for jumps: first the data is smoothened using a rolling window\n",
    "    # of radius \"jtr\", then every timestamp where the change from the\n",
    "    # previous timestamp is greater/lesser than the threshold \"jtv\" is\n",
    "    # marked, and finally a timestamp+location is marked as a hotspot\n",
    "    # if the change in the values at that location is the\n",
    "    # highest/lowest in a radius \"jsr\"\n",
    "    rolling_j = df.rolling(2*params['jtr'] + 1, min_periods=1, center=True).mean().diff()\n",
    "    res_jump_high = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "    res_jump_low = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "    for mid in df.columns:\n",
    "        neighborhood = (distances.loc[mid] <= params['jsr'] * 1000)\n",
    "        neighborhood_max = rolling_j.loc[:,neighborhood].max(axis=1)\n",
    "        neighborhood_min = rolling_j.loc[:,neighborhood].min(axis=1)\n",
    "        res_jump_high.loc[:, mid] = (rolling_j[mid] > params['jtv']).to_numpy() &\\\n",
    "        (rolling_j[mid] > neighborhood_max).to_numpy()\n",
    "        res_jump_high.loc[rolling_j[mid].isna() | neighborhood_max.isna(), mid] = np.nan\n",
    "        res_jump_low.loc[:, mid] = (rolling_j[mid] < -params['jtv']).to_numpy() &\\\n",
    "        (rolling_j[mid] < neighborhood_min).to_numpy()\n",
    "        res_jump_low.loc[rolling_j[mid].isna() | neighborhood_min.isna(), mid] = np.nan\n",
    "    res[(res_jump_high == 1) & res.notna()] += 9\n",
    "    res[(res_jump_high == 1) & res.isna()] = 9\n",
    "    res[(res_jump_low == 1) & res.notna()] += 1\n",
    "    res[(res_jump_low == 1) & res.isna()] = 1\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fe695f",
   "metadata": {},
   "source": [
    "# Parameters for Data and Spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea0bde17",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'combined'\n",
    "sensor = 'pm25'\n",
    "res_time = '3H'\n",
    "res_space = '0'\n",
    "wtr = 1\n",
    "wttf = 0.5\n",
    "wsr = 5.0\n",
    "wstf = 0.5\n",
    "jtr = 1\n",
    "jsr = 5.0\n",
    "jtv = 100 if sensor == 'pm25' else 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faad4e27",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51cdd0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_root = '/scratch/ab9738/epod-nyu-delhi-pollution/'\n",
    "filepath_data_kai = filepath_root+'data/kaiterra/kaiterra_fieldeggid_{}_current_panel.csv'.format(res_time)\n",
    "filepath_data_gov = filepath_root+'data/govdata/govdata_{}_current.csv'.format(res_time)\n",
    "filepath_locs_kai = filepath_root+'data/kaiterra/kaiterra_locations.csv'\n",
    "filepath_locs_gov = filepath_root+'data/govdata/govdata_locations.csv'\n",
    "\n",
    "locs_kai = pd.read_csv(filepath_locs_kai, index_col=[0])\n",
    "locs_kai['Type'] = 'Kaiterra'\n",
    "locs_gov = pd.read_csv(filepath_locs_gov, index_col=[0])\n",
    "locs_gov['Type'] = 'Govt'\n",
    "locs = pd.merge(locs_kai, locs_gov, how='outer',\\\n",
    "                on=['Monitor ID', 'Latitude', 'Longitude', 'Location', 'Type'], copy=False)\n",
    "data_kai = pd.read_csv(filepath_data_kai, index_col=[0,1], parse_dates=True)[sensor]\n",
    "data_gov = pd.read_csv(filepath_data_gov, index_col=[0,1], parse_dates=True)[sensor]\n",
    "data = pd.concat([data_kai, data_gov], axis=0, copy=False)\n",
    "\n",
    "start_dt = data.index.levels[1][0]\n",
    "end_dt = data.index.levels[1][-1]\n",
    "\n",
    "if start_dt.tzname != 'IST':\n",
    "        if start_dt.tzinfo is None:\n",
    "            start_dt = start_dt.tz_localize('UTC')\n",
    "        start_dt = start_dt.tz_convert(pytz.FixedOffset(330))\n",
    "    \n",
    "if end_dt.tzname != 'IST':\n",
    "    if end_dt.tzinfo is None: \n",
    "        end_dt = end_dt.tz_localize('UTC')\n",
    "    end_dt = end_dt.tz_convert(pytz.FixedOffset(330))\n",
    "\n",
    "# now, filter through the start and end dates\n",
    "data.sort_index(inplace=True)\n",
    "data = data.loc[(slice(None), slice(start_dt, end_dt))]\n",
    "\n",
    "if(source=='govdata'):\n",
    "    df = data_gov.unstack(level=0)\n",
    "elif(source=='kaiterra'):\n",
    "    df = data_kai.unstack(level=0)\n",
    "else:\n",
    "    df = data.unstack(level=0)\n",
    "distances = pd.read_csv('/scratch/ab9738/epod-nyu-delhi-pollution/data/combined_distances.csv', index_col=[0])\n",
    "distances = distances.loc[df.columns, df.columns]\n",
    "distances[distances == 0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1769dd9a",
   "metadata": {},
   "source": [
    "# Generate Spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78df1816",
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes_params = {'wtr':wtr, 'wttf':wttf, 'wsr':wsr, 'wstf':wstf, 'jtr':jtr, 'jtv':jtv, 'jsr':jsr}\n",
    "spikes = get_spikes(data, sensor, spikes_params, locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7445d1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes.to_csv('spikes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270064c7",
   "metadata": {},
   "source": [
    "# Temporal Hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ac11a851",
   "metadata": {},
   "outputs": [],
   "source": [
    "th = 150\n",
    "base_win_factor = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c0102adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_temporal_high(arr, th):\n",
    "    peak_indices = []\n",
    "    hotspot_indices = []\n",
    "    hotspot_windows = []\n",
    "    plateau = 0\n",
    "    for i in range(1,len(arr)-1):\n",
    "        if(arr[i]>arr[i-1] and arr[i]>arr[i+1]):\n",
    "            peak_indices.append(i)\n",
    "    for ind in peak_indices:\n",
    "        left_ind = ind-1\n",
    "        right_ind = ind+1\n",
    "        while(left_ind!=-1):\n",
    "            if(arr[left_ind]<arr[left_ind+1]):\n",
    "                left_ind -= 1\n",
    "            else:\n",
    "                break\n",
    "        while(right_ind!=len(arr)):\n",
    "            if(arr[right_ind]<arr[right_ind-1]):\n",
    "                right_ind += 1\n",
    "            else:\n",
    "                break\n",
    "        right_ind -= 1\n",
    "        left_ind += 1\n",
    "        if(left_ind==-1):\n",
    "            left_ind = 0\n",
    "        if(right_ind==len(arr)):\n",
    "            right_ind = len(arr)-1\n",
    "        val_left = arr[left_ind]\n",
    "        val_right = arr[right_ind]\n",
    "        val_peak = arr[ind]\n",
    "        win_size = right_ind-left_ind+1\n",
    "        base_window_left = arr[max(left_ind-base_win_factor*win_size,0):left_ind]\n",
    "        base_window_right = arr[right_ind+1:max(right_ind+base_win_factor*win_size,len(arr)-1)]\n",
    "        base_window = np.concatenate((base_window_left,base_window_right))\n",
    "        base_height = np.mean(base_window)\n",
    "#         if(val_peak-val_right>th or val_peak-val_left>th):\n",
    "#             hotspot_indices.append(ind)\n",
    "#             hotspot_windows.append((left_ind,right_ind))\n",
    "        if(val_peak-base_height>th):\n",
    "            hotspot_indices.append(ind)\n",
    "            hotspot_windows.append((left_ind,right_ind))\n",
    "        if(abs(val_right-val_left)>th):\n",
    "            plateau+=1\n",
    "    return hotspot_indices, hotspot_windows, plateau\n",
    "\n",
    "def find_temporal_low(arr, th):\n",
    "    peak_indices = []\n",
    "    hotspot_indices = []\n",
    "    hotspot_windows = []\n",
    "    plateau = 0\n",
    "    for i in range(1,len(arr)-1):\n",
    "        if(arr[i]<arr[i-1] and arr[i]<arr[i+1]):\n",
    "            peak_indices.append(i)\n",
    "    for ind in peak_indices:\n",
    "        left_ind = ind-1\n",
    "        right_ind = ind+1\n",
    "        while(left_ind!=-1):\n",
    "            if(arr[left_ind]>arr[left_ind+1]):\n",
    "                left_ind -= 1\n",
    "            else:\n",
    "                break\n",
    "        while(right_ind!=len(arr)):\n",
    "            if(arr[right_ind]>arr[right_ind-1]):\n",
    "                right_ind += 1\n",
    "            else:\n",
    "                break\n",
    "        right_ind -= 1\n",
    "        left_ind += 1\n",
    "        if(left_ind==-1):\n",
    "            left_ind = 0\n",
    "        if(right_ind==len(arr)):\n",
    "            right_ind = len(arr)-1\n",
    "        val_left = arr[left_ind]\n",
    "        val_right = arr[right_ind]\n",
    "        val_peak = arr[ind] \n",
    "        win_size = right_ind-left_ind+1\n",
    "        base_window_left = arr[max(left_ind-base_win_factor*win_size,0):left_ind]\n",
    "        base_window_right = arr[right_ind+1:max(right_ind+base_win_factor*win_size,len(arr)-1)]\n",
    "        base_window = np.concatenate((base_window_left,base_window_right))\n",
    "        base_height = np.mean(base_window)\n",
    "        if(abs(val_peak-base_height)>th):\n",
    "            hotspot_indices.append(ind)\n",
    "            hotspot_windows.append((left_ind,right_ind))\n",
    "#         if(abs(val_peak-val_right)>th or abs(val_peak-val_left)>th):\n",
    "#             hotspot_indices.append(ind)\n",
    "#             hotspot_windows.append((left_ind,right_ind))\n",
    "        if(abs(val_right-val_left)>th):\n",
    "            plateau+=1\n",
    "    return hotspot_indices, hotspot_windows, plateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "701cae9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8906 2284\n"
     ]
    }
   ],
   "source": [
    "num_hsps, num_plts = 0,0\n",
    "indices, windows = [], []\n",
    "thsp_high = {}\n",
    "for col in list(df.columns):\n",
    "    thsp_high[col] = []\n",
    "    arr = df[col].dropna().to_numpy()\n",
    "    ts = df[col].dropna().index.to_numpy()\n",
    "    hsp_ind, hsp_win, plat = find_temporal_high(arr, th)\n",
    "    for i in range(len(hsp_ind)):\n",
    "        index = ts[hsp_ind[i]]\n",
    "        value = df[col].dropna().loc[index]\n",
    "        hsp_indices = ts[hsp_win[i][0]:hsp_win[i][1]+1]\n",
    "        thsp_high[col].append([index, value, hsp_indices])\n",
    "        \n",
    "    indices += hsp_ind\n",
    "    num_hsps += len(hsp_ind)\n",
    "    num_plts += plat\n",
    "print(num_hsps, num_plts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ff8185f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2394 3158\n"
     ]
    }
   ],
   "source": [
    "num_hsps, num_plts = 0,0\n",
    "indices, windows = [], []\n",
    "thsp_low = {}\n",
    "for col in list(df.columns):\n",
    "    thsp_low[col] = []\n",
    "    arr = df[col].dropna().to_numpy()\n",
    "    ts = df[col].dropna().index.to_numpy()\n",
    "    hsp_ind, hsp_win, plat = find_temporal_low(arr, th)\n",
    "    for i in range(len(hsp_ind)):\n",
    "        index = ts[hsp_ind[i]]\n",
    "        value = df[col].dropna().loc[index]\n",
    "        hsp_indices = ts[hsp_win[i][0]:hsp_win[i][1]+1]\n",
    "        thsp_low[col].append([index, value, hsp_indices])\n",
    "        \n",
    "    indices += hsp_ind\n",
    "    num_hsps += len(hsp_ind)\n",
    "    num_plts += plat\n",
    "print(num_hsps, num_plts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ee5d7bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('time_high_3H.pkl','wb') as file:\n",
    "    pkl.dump(thsp_high, file)\n",
    "with open('time_low_3H.pkl','wb') as file:\n",
    "    pkl.dump(thsp_low, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08e980e",
   "metadata": {},
   "source": [
    "# Spatial hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5106a6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_dict = {}\n",
    "for col in df.columns:\n",
    "    dist_df = distances.loc[col].sort_values().dropna()\n",
    "    nn_dict[col] = dist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "05720cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "th = 150\n",
    "rad_th = 10\n",
    "base_rad_factor = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3d726e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def area(x1, y1, x2, y2, x3, y3):\n",
    " \n",
    "    return abs((x1 * (y2 - y3) + x2 * (y3 - y1)\n",
    "                + x3 * (y1 - y2)) / 2.0)\n",
    "\n",
    "def isInside(x1, y1, x2, y2, x3, y3, x, y):\n",
    " \n",
    "    # Calculate area of triangle ABC\n",
    "    A = area (x1, y1, x2, y2, x3, y3)\n",
    " \n",
    "    # Calculate area of triangle PBC\n",
    "    A1 = area (x, y, x2, y2, x3, y3)\n",
    "     \n",
    "    # Calculate area of triangle PAC\n",
    "    A2 = area (x1, y1, x, y, x3, y3)\n",
    "     \n",
    "    # Calculate area of triangle PAB\n",
    "    A3 = area (x1, y1, x2, y2, x, y)\n",
    "     \n",
    "    # Check if sum of A1, A2 and A3\n",
    "    # is same as A\n",
    "    if(A == A1 + A2 + A3):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def find_interior_points(hsp, pt, nnk, rem_pts):\n",
    "    hsp_cord = (locs.loc[hsp]['Latitude'],locs.loc[hsp]['Longitude'])\n",
    "    pt_cord = (locs.loc[pt]['Latitude'],locs.loc[pt]['Longitude'])\n",
    "    nnk_cord = (locs.loc[nnk]['Latitude'],locs.loc[nnk]['Longitude'])\n",
    "    interior_pts = []\n",
    "    for point in rem_pts:\n",
    "        cord = (locs.loc[point]['Latitude'],locs.loc[point]['Longitude'])\n",
    "        if(isInside(hsp_cord[0],hsp_cord[1],pt_cord[0],pt_cord[1],nnk_cord[0],nnk_cord[1],cord[0],cord[1])):\n",
    "            interior_pts.append(point)\n",
    "    return interior_pts\n",
    "\n",
    "def check_interior_condition_high(hsp, hsp_set, nnk, snap):\n",
    "#     if(snap[hsp]-snap[nnk]<th):\n",
    "#         return False\n",
    "    if(snap[hsp]-snap[nnk]<0):\n",
    "        return False\n",
    "    if(len(hsp_set)<=1):\n",
    "        return True\n",
    "    \n",
    "    for pt in hsp_set:\n",
    "        int_pts = find_interior_points(hsp, pt, nnk, [x for x in hsp_set if x != pt])\n",
    "        for intpt in int_pts:\n",
    "            if(snap[intpt]<snap[nnk] or snap[intpt]<snap[pt]):\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def check_interior_condition_low(hsp, hsp_set, nnk, snap):\n",
    "#     if(snap[hsp]-snap[nnk]>-th):\n",
    "#         return False\n",
    "    if(snap[hsp]-snap[nnk]>0):\n",
    "        return False\n",
    "    if(len(hsp_set)<=1):\n",
    "        return True\n",
    "    \n",
    "    for pt in hsp_set:\n",
    "        int_pts = find_interior_points(hsp, pt, nnk, [x for x in hsp_set if x != pt])\n",
    "        for intpt in int_pts:\n",
    "            if(snap[intpt]>snap[nnk] or snap[intpt]>snap[pt]):\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "47d08a0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7320/7320 [07:10<00:00, 16.99it/s]  \n"
     ]
    }
   ],
   "source": [
    "shsp_high = []\n",
    "for ts in tqdm(list(df.index)):\n",
    "    df_snap = df.loc[ts].dropna()\n",
    "    loc_list = list(df_snap.index)\n",
    "    for location in loc_list:\n",
    "        available_locs = [x for x in loc_list if x != location]\n",
    "        nn_locs = list(nn_dict[location][nn_dict[location]<rad_th*1000].index)\n",
    "        avg_locs = list(nn_dict[location][nn_dict[location]<base_rad_factor*rad_th*1000].index)\n",
    "        nn_list = [x for x in nn_locs if x in available_locs]\n",
    "        avg_list = [x for x in avg_locs if x in available_locs]\n",
    "        avg_reading = df_snap[avg_list].mean()\n",
    "        if(df_snap[location]-avg_reading>th):\n",
    "            hsp = location\n",
    "            hsp_set = []\n",
    "            for nnk in nn_list:\n",
    "                if(check_interior_condition_high(hsp,hsp_set,nnk,df_snap)):\n",
    "                    hsp_set.append(nnk)\n",
    "            if(len(hsp_set)):\n",
    "                shsp_high.append([ts,hsp,hsp_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "64044fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7320/7320 [04:07<00:00, 29.58it/s]\n"
     ]
    }
   ],
   "source": [
    "shsp_low = []\n",
    "for ts in tqdm(list(df.index)):\n",
    "    df_snap = df.loc[ts].dropna()\n",
    "    loc_list = list(df_snap.index)\n",
    "    for location in loc_list:\n",
    "        available_locs = [x for x in loc_list if x != location]\n",
    "        nn_locs = list(nn_dict[location][nn_dict[location]<rad_th*1000].index)\n",
    "        avg_locs = list(nn_dict[location][nn_dict[location]<base_rad_factor*rad_th*1000].index)\n",
    "        nn_list = [x for x in nn_locs if x in available_locs]\n",
    "        avg_list = [x for x in avg_locs if x in available_locs]\n",
    "        avg_reading = df_snap[avg_list].mean()\n",
    "        if(df_snap[location]-avg_reading>th):\n",
    "            hsp = location\n",
    "            hsp_set = []\n",
    "            for nnk in nn_list:\n",
    "                if(check_interior_condition_low(hsp,hsp_set,nnk,df_snap)):\n",
    "                    hsp_set.append(nnk)\n",
    "            if(len(hsp_set)):\n",
    "                shsp_low.append([ts,hsp,hsp_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "521b687a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2198"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shsp_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fad86664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "644"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shsp_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6a838ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('space_high_3H.pkl','wb') as file:\n",
    "    pkl.dump(shsp_high, file)\n",
    "with open('space_low_3H.pkl','wb') as file:\n",
    "    pkl.dump(shsp_low, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef8387e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
